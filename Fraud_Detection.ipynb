{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf10f3d",
   "metadata": {},
   "source": [
    "# 🛡️ Fraud Detection using Transaction Data\n",
    "\n",
    "This project detects fraudulent transactions using machine learning (Decision Tree Classifier).  \n",
    "We process and analyze transaction data from `.pkl` and `.csv` files.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Files Used:\n",
    "- `dataset.zip` → Contains multiple `.pkl` files with transaction data\n",
    "- `combined_transaction.csv` → All `.pkl` files combined into one CSV\n",
    "- `new_transaction.csv` → A sample transaction file for prediction\n",
    "- `fraud_detection_model.pkl` → Saved model used for final prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Steps Covered:\n",
    "1. **Extract Dataset** – Unzip and read `.pkl` files\n",
    "2. **Load & Combine** – Convert `.pkl` files to a single CSV\n",
    "3. **Preprocess** – Prepare data for modeling\n",
    "4. **Train Model** – Fit Decision Tree Classifier\n",
    "5. **Predict** – Predict fraud status using the trained model\n",
    "6. **Save Artifacts** – Store model and processed data for reuse\n",
    "\n",
    "---\n",
    "\n",
    "> Created using Python, pandas, scikit-learn, and joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inside folder: unzipped\n",
      "\n",
      "Inside folder: unzipped\\data\n",
      "  └── 2018-04-01.pkl\n",
      "  └── 2018-04-02.pkl\n",
      "  └── 2018-04-03.pkl\n",
      "  └── 2018-04-04.pkl\n",
      "  └── 2018-04-05.pkl\n",
      "  └── 2018-04-06.pkl\n",
      "  └── 2018-04-07.pkl\n",
      "  └── 2018-04-08.pkl\n",
      "  └── 2018-04-09.pkl\n",
      "  └── 2018-04-10.pkl\n",
      "  └── 2018-04-11.pkl\n",
      "  └── 2018-04-12.pkl\n",
      "  └── 2018-04-13.pkl\n",
      "  └── 2018-04-14.pkl\n",
      "  └── 2018-04-15.pkl\n",
      "  └── 2018-04-16.pkl\n",
      "  └── 2018-04-17.pkl\n",
      "  └── 2018-04-18.pkl\n",
      "  └── 2018-04-19.pkl\n",
      "  └── 2018-04-20.pkl\n",
      "  └── 2018-04-21.pkl\n",
      "  └── 2018-04-22.pkl\n",
      "  └── 2018-04-23.pkl\n",
      "  └── 2018-04-24.pkl\n",
      "  └── 2018-04-25.pkl\n",
      "  └── 2018-04-26.pkl\n",
      "  └── 2018-04-27.pkl\n",
      "  └── 2018-04-28.pkl\n",
      "  └── 2018-04-29.pkl\n",
      "  └── 2018-04-30.pkl\n",
      "  └── 2018-05-01.pkl\n",
      "  └── 2018-05-02.pkl\n",
      "  └── 2018-05-03.pkl\n",
      "  └── 2018-05-04.pkl\n",
      "  └── 2018-05-05.pkl\n",
      "  └── 2018-05-06.pkl\n",
      "  └── 2018-05-07.pkl\n",
      "  └── 2018-05-08.pkl\n",
      "  └── 2018-05-09.pkl\n",
      "  └── 2018-05-10.pkl\n",
      "  └── 2018-05-11.pkl\n",
      "  └── 2018-05-12.pkl\n",
      "  └── 2018-05-13.pkl\n",
      "  └── 2018-05-14.pkl\n",
      "  └── 2018-05-15.pkl\n",
      "  └── 2018-05-16.pkl\n",
      "  └── 2018-05-17.pkl\n",
      "  └── 2018-05-18.pkl\n",
      "  └── 2018-05-19.pkl\n",
      "  └── 2018-05-20.pkl\n",
      "  └── 2018-05-21.pkl\n",
      "  └── 2018-05-22.pkl\n",
      "  └── 2018-05-23.pkl\n",
      "  └── 2018-05-24.pkl\n",
      "  └── 2018-05-25.pkl\n",
      "  └── 2018-05-26.pkl\n",
      "  └── 2018-05-27.pkl\n",
      "  └── 2018-05-28.pkl\n",
      "  └── 2018-05-29.pkl\n",
      "  └── 2018-05-30.pkl\n",
      "  └── 2018-05-31.pkl\n",
      "  └── 2018-06-01.pkl\n",
      "  └── 2018-06-02.pkl\n",
      "  └── 2018-06-03.pkl\n",
      "  └── 2018-06-04.pkl\n",
      "  └── 2018-06-05.pkl\n",
      "  └── 2018-06-06.pkl\n",
      "  └── 2018-06-07.pkl\n",
      "  └── 2018-06-08.pkl\n",
      "  └── 2018-06-09.pkl\n",
      "  └── 2018-06-10.pkl\n",
      "  └── 2018-06-11.pkl\n",
      "  └── 2018-06-12.pkl\n",
      "  └── 2018-06-13.pkl\n",
      "  └── 2018-06-14.pkl\n",
      "  └── 2018-06-15.pkl\n",
      "  └── 2018-06-16.pkl\n",
      "  └── 2018-06-17.pkl\n",
      "  └── 2018-06-18.pkl\n",
      "  └── 2018-06-19.pkl\n",
      "  └── 2018-06-20.pkl\n",
      "  └── 2018-06-21.pkl\n",
      "  └── 2018-06-22.pkl\n",
      "  └── 2018-06-23.pkl\n",
      "  └── 2018-06-24.pkl\n",
      "  └── 2018-06-25.pkl\n",
      "  └── 2018-06-26.pkl\n",
      "  └── 2018-06-27.pkl\n",
      "  └── 2018-06-28.pkl\n",
      "  └── 2018-06-29.pkl\n",
      "  └── 2018-06-30.pkl\n",
      "  └── 2018-07-01.pkl\n",
      "  └── 2018-07-02.pkl\n",
      "  └── 2018-07-03.pkl\n",
      "  └── 2018-07-04.pkl\n",
      "  └── 2018-07-05.pkl\n",
      "  └── 2018-07-06.pkl\n",
      "  └── 2018-07-07.pkl\n",
      "  └── 2018-07-08.pkl\n",
      "  └── 2018-07-09.pkl\n",
      "  └── 2018-07-10.pkl\n",
      "  └── 2018-07-11.pkl\n",
      "  └── 2018-07-12.pkl\n",
      "  └── 2018-07-13.pkl\n",
      "  └── 2018-07-14.pkl\n",
      "  └── 2018-07-15.pkl\n",
      "  └── 2018-07-16.pkl\n",
      "  └── 2018-07-17.pkl\n",
      "  └── 2018-07-18.pkl\n",
      "  └── 2018-07-19.pkl\n",
      "  └── 2018-07-20.pkl\n",
      "  └── 2018-07-21.pkl\n",
      "  └── 2018-07-22.pkl\n",
      "  └── 2018-07-23.pkl\n",
      "  └── 2018-07-24.pkl\n",
      "  └── 2018-07-25.pkl\n",
      "  └── 2018-07-26.pkl\n",
      "  └── 2018-07-27.pkl\n",
      "  └── 2018-07-28.pkl\n",
      "  └── 2018-07-29.pkl\n",
      "  └── 2018-07-30.pkl\n",
      "  └── 2018-07-31.pkl\n",
      "  └── 2018-08-01.pkl\n",
      "  └── 2018-08-02.pkl\n",
      "  └── 2018-08-03.pkl\n",
      "  └── 2018-08-04.pkl\n",
      "  └── 2018-08-05.pkl\n",
      "  └── 2018-08-06.pkl\n",
      "  └── 2018-08-07.pkl\n",
      "  └── 2018-08-08.pkl\n",
      "  └── 2018-08-09.pkl\n",
      "  └── 2018-08-10.pkl\n",
      "  └── 2018-08-11.pkl\n",
      "  └── 2018-08-12.pkl\n",
      "  └── 2018-08-13.pkl\n",
      "  └── 2018-08-14.pkl\n",
      "  └── 2018-08-15.pkl\n",
      "  └── 2018-08-16.pkl\n",
      "  └── 2018-08-17.pkl\n",
      "  └── 2018-08-18.pkl\n",
      "  └── 2018-08-19.pkl\n",
      "  └── 2018-08-20.pkl\n",
      "  └── 2018-08-21.pkl\n",
      "  └── 2018-08-22.pkl\n",
      "  └── 2018-08-23.pkl\n",
      "  └── 2018-08-24.pkl\n",
      "  └── 2018-08-25.pkl\n",
      "  └── 2018-08-26.pkl\n",
      "  └── 2018-08-27.pkl\n",
      "  └── 2018-08-28.pkl\n",
      "  └── 2018-08-29.pkl\n",
      "  └── 2018-08-30.pkl\n",
      "  └── 2018-08-31.pkl\n",
      "  └── 2018-09-01.pkl\n",
      "  └── 2018-09-02.pkl\n",
      "  └── 2018-09-03.pkl\n",
      "  └── 2018-09-04.pkl\n",
      "  └── 2018-09-05.pkl\n",
      "  └── 2018-09-06.pkl\n",
      "  └── 2018-09-07.pkl\n",
      "  └── 2018-09-08.pkl\n",
      "  └── 2018-09-09.pkl\n",
      "  └── 2018-09-10.pkl\n",
      "  └── 2018-09-11.pkl\n",
      "  └── 2018-09-12.pkl\n",
      "  └── 2018-09-13.pkl\n",
      "  └── 2018-09-14.pkl\n",
      "  └── 2018-09-15.pkl\n",
      "  └── 2018-09-16.pkl\n",
      "  └── 2018-09-17.pkl\n",
      "  └── 2018-09-18.pkl\n",
      "  └── 2018-09-19.pkl\n",
      "  └── 2018-09-20.pkl\n",
      "  └── 2018-09-21.pkl\n",
      "  └── 2018-09-22.pkl\n",
      "  └── 2018-09-23.pkl\n",
      "  └── 2018-09-24.pkl\n",
      "  └── 2018-09-25.pkl\n",
      "  └── 2018-09-26.pkl\n",
      "  └── 2018-09-27.pkl\n",
      "  └── 2018-09-28.pkl\n",
      "  └── 2018-09-29.pkl\n",
      "  └── 2018-09-30.pkl\n"
     ]
    }
   ],
   "source": [
    "# extract_dataset.py\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extract the ZIP\n",
    "with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped\")  # extract to 'unzipped' folder\n",
    "\n",
    "# Print all files inside it\n",
    "for root, dirs, files in os.walk(\"unzipped\"):\n",
    "    print(\"\\nInside folder:\", root)\n",
    "    for file in files:\n",
    "        print(\"  └──\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined DataFrame: (1754155, 9)\n",
      "   TRANSACTION_ID         TX_DATETIME CUSTOMER_ID TERMINAL_ID  TX_AMOUNT  \\\n",
      "0               0 2018-04-01 00:00:31         596        3156      57.16   \n",
      "1               1 2018-04-01 00:02:10        4961        3412      81.51   \n",
      "2               2 2018-04-01 00:07:56           2        1365     146.00   \n",
      "3               3 2018-04-01 00:09:29        4128        8737      64.49   \n",
      "4               4 2018-04-01 00:10:34         927        9906      50.99   \n",
      "\n",
      "  TX_TIME_SECONDS TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \n",
      "0              31            0         0                  0  \n",
      "1             130            0         0                  0  \n",
      "2             476            0         0                  0  \n",
      "3             569            0         0                  0  \n",
      "4             634            0         0                  0  \n",
      "Combined data saved to 'combined_transactions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load_data.py\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the .pkl files\n",
    "folder_path = 'unzipped/data'\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f\"Error: Folder path {folder_path} does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# List to store individual DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Load all .pkl files from the folder\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith('.pkl'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Read the .pkl file and append to the dataframes list\n",
    "            df = pd.read_pickle(file_path)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames into one\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Display the combined DataFrame's shape and first few rows\n",
    "    print(\"Shape of combined DataFrame:\", combined_df.shape)\n",
    "    print(combined_df.head())\n",
    "\n",
    "    # Save the combined data into a CSV file\n",
    "    combined_df.to_csv('combined_transactions.csv', index=False)\n",
    "    print(\"Combined data saved to 'combined_transactions.csv'.\")\n",
    "else:\n",
    "    print(\"No valid .pkl files found to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590512f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1403324, 3)\n",
      "Testing Features Shape: (350831, 3)\n",
      "Training Labels Shape: (1403324,)\n",
      "Testing Labels Shape: (350831,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess_data.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the saved data\n",
    "data = pd.read_csv('combined_transactions.csv')\n",
    "\n",
    "# Step 2: Select features (input columns) and target (what you want to predict)\n",
    "features = ['TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS']  # Example features\n",
    "X = data[features]\n",
    "y = data['TX_FRAUD']\n",
    "\n",
    "# Step 3: Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Print shapes to check\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Testing Features Shape: {X_test.shape}\")\n",
    "print(f\"Training Labels Shape: {y_train.shape}\")\n",
    "print(f\"Testing Labels Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26dac44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n",
      "\n",
      "Confusion Matrix:\n",
      "[[345578   2392]\n",
      " [  2198    663]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    347970\n",
      "           1       0.22      0.23      0.22      2861\n",
      "\n",
      "    accuracy                           0.99    350831\n",
      "   macro avg       0.61      0.61      0.61    350831\n",
      "weighted avg       0.99      0.99      0.99    350831\n",
      "\n",
      "\n",
      "Model saved as 'fraud_detection_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# train_model.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Step 1: Load the saved CSV\n",
    "data = pd.read_csv('combined_transactions.csv')\n",
    "\n",
    "# Step 2: Define features (X) and label (y)\n",
    "features = ['TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS']  # Same as before\n",
    "X = data[features]\n",
    "y = data['TX_FRAUD']\n",
    "\n",
    "# Step 3: Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Create and train the model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Save the model (optional, for later use)\n",
    "import joblib\n",
    "joblib.dump(model, 'fraud_detection_model.pkl')\n",
    "print(\"\\nModel saved as 'fraud_detection_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716295b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "New transactions:\n",
      "   TX_AMOUNT  TX_TIME_SECONDS  TX_TIME_DAYS\n",
      "0     100.50             3600             0\n",
      "1       5.75             7200             1\n",
      "2    3000.00            10800             2\n",
      "Transaction 1: Not Fraud\n",
      "Transaction 2: Not Fraud\n",
      "Transaction 3: Fraud\n"
     ]
    }
   ],
   "source": [
    "# predict.py\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load the saved model\n",
    "model = joblib.load('fraud_detection_model.pkl')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Step 2: Create some new sample transactions\n",
    "# (You can change these values to test different transactions!)\n",
    "new_transactions = pd.DataFrame({\n",
    "    'TX_AMOUNT': [100.50, 5.75, 3000.00],\n",
    "    'TX_TIME_SECONDS': [3600, 7200, 10800],\n",
    "    'TX_TIME_DAYS': [0, 1, 2]\n",
    "})\n",
    "\n",
    "print(\"\\nNew transactions:\")\n",
    "print(new_transactions)\n",
    "\n",
    "# Step 3: Make predictions\n",
    "predictions = model.predict(new_transactions)\n",
    "\n",
    "# Step 4: Show the predictions\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    result = \"Fraud\" if prediction == 1 else \"Not Fraud\"\n",
    "    print(f\"Transaction {idx+1}: {result}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
